import pandas as pd
import spacy
import re
from textblob import TextBlob
import enchant
import string
from textures.find import *
from langdetect import detect
import textstat

def extract_features(df, text_col='text',
                     use_spacy_features=True,
                     use_emoji_features=True,
                     use_misspelling_features=True,
                     n_hashtags=True,
                     n_unique_hashtags=True,
                     n_mentions=True,
                     n_unique_mentions=True,
                     n_words=True,
                     n_unique_words=True,
                     n_stopwords = True,
                     n_characters=True,
                     n_unique_characters=True,
                     n_unique_urls=True,
                     n_upper=True,
                     n_lower=True,
                     n_numbers = True,
                     n_puncts=True,
                     n_exclaims=True,
                     n_extraspace=True,
                     n_title=True,
                     detect_lang=True,
                     readability_score=True
                     ):
    """
    Computes and returns several handy features and stats from a text column in a Pandas dataframe.

    Parameters
    ----------
    :param df: DataFrame
    :param text_col: label, text column label
    :param use_spacy_features: bool, default True, if True computes and returns features generated by Spacy,incluing n_tokens,n_unique_tokens, n_entities, and n_stopwords
    :param use_emoji_features: bool, default True, if True returns the number of emojis.
    :param use_misspelling_features: bool, default True, if True returns the number of misspelled words. Behind the scene it uses the enchant python library.
    :param n_hashtags: bool, default True, if True returns the number of hashtags (tokens starting with #).
    :param n_unique_hashtags: bool, default True, if True returns the number of unique hashtags (tokens starting with #).
    :param n_mentions: bool, default True, if True returns the number of mentions (tokens starting with @).
    :param n_unique_mentions: bool, default True, if True returns the number of unique mentions (tokens starting with @).
    :param n_words: bool, default True, if True returns the number of words found in the text column.
    :param n_unique_words:  bool, default True, if True returns the number of unique words.
    :param n_stopwords:  bool, default True, if True returns the number of stop words.
    :param n_characters: bool, default True, if True returns the number of characters.
    :param n_unique_characters: bool, default True, if True returns the number of unique characters.
    :param n_unique_urls: bool, default True, if True returns the number of unique URLs.
    :param n_upper: bool, default True, if True returns the total number of all caps (all capital letters) or upper case words.
    :param n_lower: bool, default True, if True returns the total number of lower case words.
    :param n_title: bool, default True, if True returns the total number of title case words.
    :param n_numbers: bool, default True, if True returns the total number of numbers found in the text column.
    :param n_puncts: bool, default True, if True returns the total number of punctuations.
    :param n_exclaims: bool, default True, if True returns the total number of exclamations.
    :param n_extraspace: bool, default True, if True returns the total number of extra spaces in the text column.
    :param detect_lang: bool, default True, if True detects and returns the language of text.
    :param readability_score: bool, default True, if True returns the  Flesch readability score of the text column.

    Returns
    -------
    DataFrame
    A DataFrame of the two merged objects.

    Examples
    --------
    >>> df = pd.DataFrame({'text':['A BRown #fox jumps over the  lazzzy   dog!ðŸ¶ðŸ˜’ @lazydog']})
    >>> extract_features(df).shape
    (1, 34)
    """
    if use_emoji_features:
        df['n_emojis'] = df[text_col].apply(lambda x: len(find_emojis(x)))
    if use_spacy_features:
        nlp = spacy.load('en_core_web_sm')
        df['nlp'] = df[text_col].apply(lambda x: nlp(x, disable=['tagger', 'parser']))
        df['n_entities'] = df['nlp'].apply(lambda x: len([ent for ent in x.ents]))
        df['tokens'] = df['nlp'].apply(lambda x: [token.text for token in x])
        df['n_stopwords'] = df['nlp'].apply(lambda x: sum([token.is_stop for token in x]))
        df['n_tokens'] = df['tokens'].apply(lambda x: len(x))
        df['n_unique_tokens'] = df['tokens'].apply(lambda x: len(set(x)))
    first_person_pronouns = ["i", "me", "myself", "my", "mine", "this"]
    first_personp_pronouns = ["we", "us", "our", "ours", "these"]
    second_person_pronouns = ["you", "yours", "your", "yourself"]
    third_person_pronouns = ["he", "she", "it", "its", "his", "hers"]
    third_personp_pronouns = ["they", "them", "theirs", "their", "they're", "their's", "those", "that"]
    to_be_verbs = ["am", "is", "are", "was", "were", "being",
                   "been", "be", "were", "be"]
    prepositions = ["about", "below", "excepting", "off", "toward", "above", "beneath",
                    "on", "under", "across", "from", "onto", "underneath", "after", "between",
                    "in", "out", "until", "against", "beyond", "outside", "up", "along", "but",
                    "inside", "over", "upon", "among", "by", "past", "around", "concerning",
                    "regarding", "with", "at", "despite", "into", "since", "within", "down",
                    "like", "through", "without", "before", "during", "near", "throughout",
                    "behind", "except", "of", "to", "for"]
    df['textblob'] = df[text_col].apply(lambda x: TextBlob(x))
    df['textblobwords'] = df['textblob'].apply(lambda x: x.words)
    df['n_words'] = df['textblobwords'].apply(lambda x: len(x))
    df['n_unique_words'] = df['textblobwords'].apply(lambda x: len(set(x)))
    df['n_sentences'] = df['textblob'].apply(lambda x: len(x.sentences))
    if use_misspelling_features:
        dict = enchant.Dict("en_US")
        df['n_misspelled_words'] = df['textblobwords'].apply(
            lambda x: len([word for word in x if dict.check(word) == False]))
    df['first_personp'] = df['textblobwords'].apply(
        lambda x: len([word for word in x if word.lower() in first_personp_pronouns]))
    df['first_personp'] = df['textblobwords'].apply(
        lambda x: len([word for word in x if word.lower() in first_personp_pronouns]))
    df['second_person_pronouns'] = df['textblobwords'].apply(
        lambda x: len([word for word in x if word.lower() in second_person_pronouns]))
    df['third_person'] = df['textblobwords'].apply(
        lambda x: len([word for word in x if word.lower() in third_person_pronouns]))
    df['third_personp'] = df['textblobwords'].apply(
        lambda x: len([word for word in x if word.lower() in third_personp_pronouns]))
    df['n_to_be'] = df['textblobwords'].apply(lambda x: len([word for word in x if word.lower() in to_be_verbs]))
    df['n_prepositions'] = df['textblobwords'].apply(
        lambda x: len([word for word in x if word.lower() in prepositions]))
    df['polarity'] = df['textblob'].apply(lambda x: x.sentiment.polarity)
    df['subjectivity'] = df['textblob'].apply(lambda x: x.sentiment.subjectivity)
    if n_hashtags == True:
        df['n_hashtags'] = df[text_col].apply(lambda x: len(find_hashtags(x)))
    if n_unique_hashtags == True:
        df['n_unique_hashtags'] = df[text_col].apply(lambda x: len(set(find_hashtags(x))))
    if n_mentions == True:
        df['n_mentions'] = df[text_col].apply(lambda x: len(find_mentions(x)))
    if n_unique_mentions == True:
        df['n_unique_mentions'] = df[text_col].apply(lambda x: len(set(find_mentions(x))))
    if n_characters:
        df['n_characters'] = df[text_col].apply(lambda x: len(x))
    if n_unique_characters:
        df['n_unique_characters'] = df[text_col].apply(lambda x: len(set([char for char in x])))
    if n_unique_urls:
        df['n_unique_urls'] = df[text_col].apply(lambda x: len(re.findall(r'([\w0-9._-]+@[\w0-9._-]+\.[\w0-9_-]+)', x)))
    if n_upper:
        df['n_upper'] = df[text_col].apply(lambda x: len(find_upper(x)))
    if n_lower:
        df['n_lower'] = df[text_col].apply(lambda x: len(find_lower(x)))
    if n_numbers:
        df['n_numbers'] = df[text_col].apply(lambda x: len(find_numbers(x)))
    if n_puncts:
        df['n_puncts'] = df[text_col].apply(lambda x: len([c for c in x if c in string.punctuation]))
    if n_exclaims:
        df['n_exclaims'] = df[text_col].apply(lambda x: len(re.findall(r'[!]', x)))
    if n_extraspace:
        df['n_extraspace'] = df[text_col].apply(lambda x: len(re.findall('  +', x)))
    if n_title:
        df['n_title'] = df[text_col].apply(lambda x: len(find_title(x)))
    if detect_lang:
        df['language'] = df[text_col].apply(lambda x: detect(x))
    if readability_score:
        df['readability_score'] = df[text_col].apply(lambda x: textstat.flesch_reading_ease(x))
    df = df.drop(['nlp', 'tokens', 'textblob', 'textblobwords' ], axis=1)
    return df
